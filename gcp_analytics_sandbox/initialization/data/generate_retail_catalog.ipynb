{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not actually ran here but included because it's what I used to make some of our the files for our base \n",
    "# mock data/initial denormalized tables (mainly the OLTP product catalog at it's related tables)\n",
    "\n",
    "# we generate our product catalog tables by extracting and transforming the top 500 results from a \n",
    "# public boardgame dataset which can be found at: https://www.kaggle.com/datasets/jvanelteren/boardgamegeek-reviews\n",
    "# whose original source datasets can be found at https://github.com/beefsack/bgg-ranking-historicals\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "\n",
    "# Create a UUID object from our seed string to act as the base namespace\n",
    "NAMESPACE_SEED = 'com.gcp.dagster.analytics.sandbox.generation.v1'\n",
    "BASE_NAMESPACE = uuid.uuid5(uuid.NAMESPACE_DNS, NAMESPACE_SEED)\n",
    "# further differentiated namespaces\n",
    "NAMESPACE_PRODUCTS = uuid.uuid5(BASE_NAMESPACE, 'boardgame_products')\n",
    "NAMESPACE_AUTHORS = uuid.uuid5(BASE_NAMESPACE, 'boardgame_authors')\n",
    "NAMESPACE_SUPPLIERS = uuid.uuid5(BASE_NAMESPACE, 'boardgame_suppliers')\n",
    "NAMESPACE_PUBLISHERS = uuid.uuid5(BASE_NAMESPACE, 'boardgame_publishers')\n",
    "NAMESPACE_PROMOTIONS = uuid.uuid5(BASE_NAMESPACE, 'boardgame_promotions')\n",
    "NAMESPACE_CHANNELS = uuid.uuid5(BASE_NAMESPACE, 'boardgame_channels')\n",
    "NAMESPACE_PAYMENT_TYPES = uuid.uuid5(BASE_NAMESPACE, 'boardgame_payment_types')\n",
    "\n",
    "# assuming you downloaded the csv to current directory\n",
    "original_games = pl.read_csv(Path.cwd().joinpath(\"games_detailed_info2025.csv\"))\n",
    "# pick only the fields we can make use of\n",
    "original_games = original_games.select([\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"description\",\n",
    "    \"yearpublished\",\n",
    "    \"thumbnail\",\n",
    "    \"image\",\n",
    "    \"boardgamecategory\",\n",
    "    \"boardgamedesigner\",\n",
    "    \"boardgamepublisher\",\n",
    "    \"minage\",\n",
    "    \"minplaytime\",\n",
    "    \"maxplaytime\",\n",
    "    \"minplayers\",\n",
    "    \"maxplayers\",\n",
    "    \"Board Game Rank\",\n",
    "    \"usersrated\",\n",
    "    \"average\"])\n",
    "\n",
    "# pick the top 500 boardgames by populatity/number of user ratings\n",
    "original_games = original_games.top_k(500, by=\"usersrated\")\n",
    "\n",
    "custom_topk = original_games.rename({\n",
    "    \"id\": \"boardgamegeek_id\",\n",
    "    \"Board Game Rank\": \"rating_rank\",                            \n",
    "    \"boardgamecategory\": \"category\",\n",
    "    \"boardgamepublisher\": \"publishers\",\n",
    "    \"yearpublished\": \"year_published\",\n",
    "    \"boardgamedesigner\": \"authors\",\n",
    "    })\n",
    "\n",
    "set_date = datetime(2025,8,1).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#setting base msrp between $20-60\n",
    "random_msrp = np.random.uniform(20.00, 60.00, size=len(custom_topk))\n",
    "random_msrp = np.round(random_msrp, 4)\n",
    "custom_topk = custom_topk.with_columns(\n",
    "    pl.Series(\"msrp\", random_msrp)\n",
    ")\n",
    "custom_topk = custom_topk.with_columns([\n",
    "    (pl.col(\"msrp\")*0.5).alias(\"cost_price\"),\n",
    "    pl.col(\"msrp\").alias(\"unit_price\"), \n",
    "    pl.lit(\"Active\").alias(\"status\"), \n",
    "    pl.lit(set_date).alias(\"date_added\"), \n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    "    ]).with_row_index(\"product_id\", 1)\n",
    "\n",
    "# using regular expressions to parse string columns into list of strings, would recommend using json_decode() \n",
    "# if conditions permit (not possible here because some fields have multiple leading qoutation marks)\n",
    "custom_topk = custom_topk.with_columns(\n",
    "    pl.col(\"category\").str.extract_all(r\"'([^']*)'\")\n",
    "    .list.eval(pl.element().str.strip_chars(\"'\")).alias(\"category\"), #strip_chars() only replaces leading and trailing chars\n",
    "    pl.col(\"authors\").str.extract_all(r\"'([^']*)'\")\n",
    "    .list.eval(pl.element().str.strip_chars(\"'\")).alias(\"authors\"),\n",
    "    pl.col(\"publishers\").str.extract_all(r\"'([^']*)'\")\n",
    "    .list.eval(pl.element().str.strip_chars(\"'\")).alias(\"publishers\"),\n",
    ")\n",
    "\n",
    "# I used ML classification to seperate all the products into 4 suppliers based on product descriptions and\n",
    "# categories and the classification essentially outputted a bridge table, but there is currently no \n",
    "# many-to-many relationship between suppliers and products in our example logic so, avoiding a \n",
    "# premature/unnecessary bridge table, the classification table is going to be joined back into the products\n",
    " \n",
    "classification_filepath = Path.cwd().joinpath(\"product_suppliers_rationale.json\")\n",
    "join_supplier = pl.read_json(classification_filepath).select(\"product_id\", \"supplier_id\")\n",
    "custom_topk = custom_topk.join(join_supplier, on=\"product_id\")\n",
    "\n",
    "# but we also could have just randomly split the products into different suppliers instead, e.g.:\n",
    "# custom_topk = custom_topk.with_columns(pl.int_uniform(1, 4).alias(\"supplier_id\"))\n",
    "# by uncommenting the above line and commenting out the earlier load classification & join lines \n",
    "\n",
    "# optional: split off the supplier_id and product_id as a bridge table\n",
    "custom_topk.select([\n",
    "    \"supplier_id\",\n",
    "    \"product_id\",\n",
    "    \"date_added\",\n",
    "    \"date_removed\",\n",
    "    \"date_modified\",\n",
    "    \"date_deleted\",]).write_parquet(Path.cwd() / \"mock_data\" / \"suppliers_products.parquet\")\n",
    "\n",
    "# reverse-engineer a normalized publisher table and bridge from the denormalized product table\n",
    "explode_publishers = custom_topk.select([\"product_id\",\"publishers\"])\n",
    "# explode the nested fields in the publishers column with their product_id for joining on publisher name\n",
    "explode_publishers = explode_publishers.explode(\"publishers\")\n",
    "# clean and filter out some malformed inputs, based on looking through the data\n",
    "explode_publishers = explode_publishers.with_columns(\n",
    "    pl.col(\"publishers\").str.strip_chars(' ,\"')\n",
    "    .str.replace_all(r'\", \"', \" \")\n",
    "    .str.replace_all(r\"^s \", \"\")\n",
    "    .alias(\"publishers\")\n",
    ").filter(\n",
    "    ~((pl.col(\"publishers\").str.starts_with(\"t \")) |\n",
    "      (pl.col(\"publishers\") == \"s\") |\n",
    "      (pl.col(\"publishers\") == \"\")\n",
    "    )\n",
    ")\n",
    "# find unique publishers and assign them an publisher_id\n",
    "unique_publishers = explode_publishers.select(\n",
    "    pl.col(\"publishers\").unique()\n",
    ").sort(\"publishers\").with_row_index(\"publisher_id\", 1)\n",
    "# join exploded products +publisher table and unique publishers to build a normalized bridge table\n",
    "unique_publishers = unique_publishers.rename({\"publishers\": \"publisher_name\"})\n",
    "explode_publishers = explode_publishers.rename({\"publishers\": \"publisher_name\"})\n",
    "explode_publishers = explode_publishers.join(unique_publishers, on=\"publisher_name\").sort(\"publisher_id\")\n",
    "explode_publishers = explode_publishers.select([\"product_id\", \"publisher_id\"]).with_columns(\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ")\n",
    "# create an uuid column for the tables that make sense having a uuid\n",
    "unique_publishers = unique_publishers.with_columns(\n",
    "    pl.col(\"publisher_id\").cast(pl.String) # Cast the integer index to a string\n",
    "            .map_elements(\n",
    "                lambda idx_str: str(uuid.uuid5(NAMESPACE_PUBLISHERS, idx_str)),\n",
    "                return_dtype=str\n",
    "            ).alias(\"publisher_uuid\"),\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ")\n",
    "# write out files as parquet\n",
    "unique_publishers.write_parquet(Path.cwd() / \"mock_data\" / \"publishers.parquet\")\n",
    "explode_publishers.write_parquet(Path.cwd() / \"mock_data\" / \"publishers_products.parquet\")\n",
    "\n",
    "# reverse-engineer a normalized category table and bridge table from the denormalized product table\n",
    "explode_categories = custom_topk.select([\"product_id\",\"category\"])\n",
    "explode_categories = explode_categories.explode(\"category\")\n",
    "# clean and filter out some malformed inputs\n",
    "explode_categories = explode_categories.with_columns(\n",
    "    pl.col(\"category\").str.strip_chars(' ,\"')\n",
    ").filter(~(\n",
    "    (pl.col(\"category\") == \"\") |\n",
    "    (pl.col(\"category\").str.starts_with(\"s \"))\n",
    "))\n",
    "# find unique categories and assign them an id\n",
    "unique_categories = explode_categories.select(\n",
    "    pl.col(\"category\").unique()\n",
    ").sort(\"category\").with_row_index(\"category_id\", 1)\n",
    "# explode the nested fields in the categories column with their product_id for joining on category name\n",
    "# to build a normalized bridge table with category_id\n",
    "explode_categories = explode_categories.join(unique_categories, on=\"category\").sort(\"category_id\").select(\"category_id\",\"product_id\").with_columns(\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ")\n",
    "# write out files as parquet\n",
    "unique_categories = unique_categories.rename({\"category\": \"category_name\"}).with_columns(\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ")\n",
    "unique_categories.write_parquet(Path.cwd() / \"mock_data\" / \"categories.parquet\")\n",
    "explode_categories.write_parquet(Path.cwd() / \"mock_data\" / \"categories_products.parquet\")\n",
    "\n",
    "# doing the same as above with authors\n",
    "# reverse-engineer a normalized authors table and bridge from the denormalized product table\n",
    "explode_authors = custom_topk.select([\"product_id\",\"authors\"])\n",
    "explode_authors = explode_authors.explode(\"authors\")\n",
    "# clean and filter out some malformed inputs\n",
    "explode_authors = explode_authors.with_columns(\n",
    "    pl.col(\"authors\").str.strip_chars(' ,\"')\n",
    ").filter(~(pl.col(\"authors\") == \"\"))\n",
    "# explode the nested fields in the authors column to find unique authors and assign them an id\n",
    "unique_authors = explode_authors.select(\n",
    "    pl.col(\"authors\").unique()\n",
    ").sort(\"authors\").with_row_index(\"author_id\", 1)\n",
    "# explode the nested fields in the authors column with their product_id for joining on authors name\n",
    "# to build a normalized bridge table with authors_id\n",
    "explode_authors = explode_authors.join(unique_authors, on=\"authors\").sort(\"author_id\").select(\"author_id\",\"product_id\").with_columns(\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ")\n",
    "unique_authors = unique_authors.rename({\"authors\": \"author_name\"})\n",
    "# create an uuid column for the tables that make sense having a uuid\n",
    "unique_authors = unique_authors.with_columns(\n",
    "    pl.col(\"author_id\").cast(pl.String) # Cast the integer index to a string\n",
    "            .map_elements(\n",
    "                lambda idx_str: str(uuid.uuid5(NAMESPACE_AUTHORS, idx_str)),\n",
    "                return_dtype=str\n",
    "            ).alias(\"author_uuid\"),\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ")\n",
    "# write out files as parquet\n",
    "unique_authors.write_parquet(Path.cwd() / \"mock_data\" / \"authors.parquet\")\n",
    "explode_authors.write_parquet(Path.cwd() / \"mock_data\" / \"authors_products.parquet\")\n",
    "\n",
    "# create an uuid column for the tables that make sense having a uuid\n",
    "custom_topk = custom_topk.with_columns(\n",
    "    pl.col(\"product_id\").cast(pl.String) # Cast the integer index to a string\n",
    "            .map_elements(\n",
    "                lambda idx_str: str(uuid.uuid5(NAMESPACE_PRODUCTS, idx_str)),\n",
    "                return_dtype=str\n",
    "            ).alias(\"product_uuid\")\n",
    ")\n",
    "\n",
    "# order columns\n",
    "custom_topk = custom_topk.select([\n",
    "    \"product_id\",\n",
    "    \"product_uuid\",\n",
    "    \"name\",\n",
    "    \"description\",\n",
    "    \"year_published\",\n",
    "    # \"supplier_id\", # splitting it out as a bridge table\n",
    "    \"boardgamegeek_id\",\n",
    "    \"minage\",\n",
    "    \"minplaytime\",\n",
    "    \"maxplaytime\",\n",
    "    \"minplayers\",\n",
    "    \"maxplayers\",\n",
    "    \"rating_rank\",\n",
    "    \"usersrated\",\n",
    "    \"average\",\n",
    "    \"thumbnail\",\n",
    "    \"image\",\n",
    "    \"cost_price\",\n",
    "    \"msrp\",\n",
    "    \"unit_price\",\n",
    "    \"status\",\n",
    "    \"date_added\",\n",
    "    \"date_removed\",\n",
    "    \"date_modified\",\n",
    "    \"date_deleted\",\n",
    "])\n",
    "\n",
    "output_filepath = Path.cwd() / \"mock_data\" / \"products\"\n",
    "custom_topk.write_parquet(f\"{output_filepath}.parquet\")\n",
    "# custom_topk.write_json(f\"{output_filepath}.json\")\n",
    "# custom_topk.write_csv(f\"{output_filepath}.csv\")\n",
    "\n",
    "# come up with a few fictional suppliers, I just wrote them out manually to ensure they're cohesive but you \n",
    "# could use the Faker library to generate most of the fields, but if you go that route I'd recommend generating the \n",
    "# whole address at once (don't forget to set different locales/country for your Faker instances) then parse the \n",
    "# individual fields like province, city, street_address, postal code from the full address as opposed to\n",
    "# generating each of those individually as Faker doesn't remember the province when generating the city, city\n",
    "# for street address, etc. - so it will not be cohesive\n",
    "\n",
    "supplier_name = [\"Euro-Stratego Distribution\", \"American Adventures Inc.\", \"Party Game Emporium\", \"Classic & Abstract Co.\"]\n",
    "# description was used for product->supplier classification, but not relevant if you just randomly assigned \n",
    "# products to suppliers\n",
    "description = [\n",
    "    \"Specializes in deep, strategic Eurogames focusing on economics, city-building, and resource management.\",\n",
    "    \"Focuses on highly thematic games with strong player interaction, miniatures, and adventure elements.\",\n",
    "    \"Supplies light, social, and family-friendly games perfect for larger groups and casual players.\",\n",
    "    \"Distributes timeless abstract strategy games, puzzle games, and updated classics for all ages.\"\n",
    "]\n",
    "country = [\"Germany\", \"USA\", \"Canada\", \"UK\"]\n",
    "cities = [\"Hamburg\", \"Long Beach\", \"Vancouver\", \"London\" ]\n",
    "provinces = [\"Hamburg\", \"California\", \"British Columbia\", \"London\"]\n",
    "addresses = [\n",
    "    \"Lagerhaus-Straße 17\",\n",
    "    \"789 Ocean Gateway Blvd\",\n",
    "    \"456 Waterfront St\",\n",
    "    \"Unit 12, Thames Gateway Park, River Road, Barking\"\n",
    "]\n",
    "postal_code = [\"21129\",\"90802\",\"V6C 3T4\",\"IG11 0JG\"]\n",
    "phone_number = [\"+49 40 12345678\",\"+1 (562) 555-0187\",\"+1 (604) 555-0123\",\"+44 20 7946 0831\"]\n",
    "names = [\"Klaus Müller\",\"Jennifer Chen\",\"David Singh\",\"Eleanor Davies\"]\n",
    "email = [\"k.muller@hamburg-logistik.de\",\"j.chen@pacificgatewaylogistics.com\",\"david.singh@vancouverportalservices.ca\",\"eleanor.davies@thamesfreight.co.uk\"]\n",
    "\n",
    "\n",
    "suppliers_dict = {\n",
    "    \"supplier_uuid\": [str(uuid.uuid5(NAMESPACE_SUPPLIERS, str(i))) for i in range(1,len(supplier_name)+1)],\n",
    "    \"supplier_name\" : supplier_name,\n",
    "    \"description\": description,\n",
    "    \"country\": country,\n",
    "    \"city\": cities,\n",
    "    \"province\": provinces,\n",
    "    \"address\": addresses,\n",
    "    \"postal_code\": postal_code,\n",
    "    \"contact_name\": names,\n",
    "    \"contact_number\": phone_number,\n",
    "    \"contact_email\": email,\n",
    "    \"date_added\": [set_date] * len(supplier_name),\n",
    "    \"date_removed\": [None] * len(supplier_name),\n",
    "    \"date_modified\": [set_date] * len(supplier_name),\n",
    "    \"date_deleted\": [None] * len(supplier_name)\n",
    "}\n",
    "\n",
    "# create an uuid column for the tables that make sense having a uuid\n",
    "suppliers = pl.DataFrame(suppliers_dict).with_row_index(\"supplier_id\",1)\n",
    "suppliers.write_parquet(Path.cwd() / \"mock_data\" / \"suppliers.parquet\")\n",
    "# suppliers.write_csv(Path.cwd() / \"mock_data\" / \"suppliers.csv\")\n",
    "\n",
    "description_schema = [\"promotion_uuid\",\"promotion_name\",\"promotion_code\",\"description\"]\n",
    "promo_descriptions = [\n",
    "    (\"5 Off Item\", \"5OFFITEM\", \"Get $5.00 off a single item.\"),\n",
    "    (\"10 Off Item Over 50\", \"10OFF50ITEM\", \"Get $10.00 off a single item with a price of $50.00 or more.\"),\n",
    "    (\"10% Off Item\", \"10PERCENTITEM\", \"Get 10% off a single item.\"),\n",
    "    (\"15% Off Item\", \"15PERCENTITEM\", \"Get 15% off a single item.\"),\n",
    "    (\"20% Off Top 10 products from 2024 \", \"20PCTTOP2024\", \"Get 20% off items from the bestsellers from 2024.\"),\n",
    "    (\"Buy One Get One 50% Off\", \"BOGO50PCT\", \"Buy one item, get the second item of equal or lesser value for 50% off.\"),\n",
    "    (\"Buy One Get One Free on Minatures\", \"BOGOFREEMINI\", \"Buy one item from the Minatures group, get a second item from the group of equal or lesser value for free.\"),\n",
    "    (\"Select Items for 20\", \"PARTY20\", \"Get Party Games products for a fixed price of $20.00.\"),\n",
    "    (\"15 Off Orders Over 100\", \"TOTAL15FLAT\", \"Get $15.00 off your total order of $100.00 or more.\"),\n",
    "    (\"15% Off Orders Over 100\", \"TOTAL15PCT\", \"Get 15% off your total order of $100.00 or more.\"),\n",
    "]\n",
    "# made the initial list a list of tuples, before realiziing I'd probably mutate the fields\n",
    "promo_descriptions = [list(item) for item in promo_descriptions]\n",
    "\n",
    "for i,e in enumerate(promo_descriptions):\n",
    "    e.insert(0, str(uuid.uuid5(NAMESPACE_PROMOTIONS, str(i))))\n",
    "\n",
    "\n",
    "promo_schema = [\"discount_level\",\"discount_pool\",\"stackable\", \"priority\", \"discount_type\",\n",
    "                 \"discount_value\", \"min_quantity\", \"min_subtotal\", \"promotion_group_id\"]\n",
    "# level, pool, stackable, priority, type, value, min_quantity, min_subtotal, promo_group\n",
    "promotions_core = [\n",
    "    ### line items level\n",
    "    # basic flat discount \n",
    "    (\"LINE_ITEM\", \"FLAT\", True, 10, \"FLAT\", 5.00, 1, 0.00, None),\n",
    "    (\"LINE_ITEM\", \"FLAT\", True, 10, \"FLAT\", 10.00, 1, 50.00, None),\n",
    "    # basic percentage discounts\n",
    "    (\"LINE_ITEM\",\"PERCENT\", True, 20, \"PERCENT\", 10.00, 1, 0.00, None),\n",
    "    (\"LINE_ITEM\",\"PERCENT\", True, 20, \"PERCENT\", 15.00, 1, 0.00, None),\n",
    "    # percentage based on product group\n",
    "    (\"LINE_ITEM\",\"PERCENT\", True, 20, \"PERCENT\", 20.00, 1, 0.00, 1),\n",
    "    # basic BOGO 50% off\n",
    "    (\"LINE_ITEM\", 'BOGO', True, 10, \"PERCENT\", 50.00, 2, 0.00, None),\n",
    "    # BOGO free based on product group\n",
    "    (\"LINE_ITEM\", 'BOGO', True, 10, \"PERCENT\", 100.00, 2, 0.00, 2),\n",
    "    # fixed price discount, based on product group\n",
    "    (\"LINE_ITEM\", \"FIXED\", True, 10, \"FIXED\", 20.00, 1, 0.00, 3),\n",
    "    ### transaction/order total level\n",
    "    # basic flat, req. min $100 subtotal\n",
    "    (\"ORDER\", \"FLAT\", True, 30, \"FLAT\", 15.00, 1, 100.00, None),\n",
    "    # basic percentage, req. min $100 subtotal\n",
    "    (\"ORDER\",\"PERCENT\", True, 30, \"PERCENT\", 15.00, 1, 100.00, None),\n",
    "]\n",
    "# made the initial list a list of tuples, before realiziing I'd probably mutate the fields\n",
    "promotions_core = [list(item) for item in promotions_core]\n",
    "combined_promos_schema = description_schema\n",
    "combined_promos_schema.extend(promo_schema)\n",
    "combined_promos = []\n",
    "for i,_ in enumerate(promo_descriptions):\n",
    "    combined_promos.append(promo_descriptions[i] + promotions_core[i])\n",
    "\n",
    "promos_core = pl.DataFrame(combined_promos, schema=combined_promos_schema,orient=\"row\").with_row_index(\"promotion_id\", 1).with_columns(\n",
    "    pl.lit(\"Active\").alias(\"status\"),\n",
    "    pl.lit(set_date).alias(\"start_date\"),\n",
    "    pl.lit(None).alias(\"end_date\"),\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ")\n",
    "# promos_core.cast({\"promotion_group_id\": pl.UInt32})\n",
    "promos_core.write_parquet(Path.cwd() / \"mock_data\" / \"promotions.parquet\")\n",
    "\n",
    "# name, type, description, category_id\n",
    "promo_groups_schema = [\"group_name\",\"group_type\",\"description\",\"category_id\"]\n",
    "promo_groups = [\n",
    "    (\"Top 10 boardgames of the past year (2024)\", \"Custom\", \"Contains a curated list of the top 10 selling products of the previous year (2024)\", None),\n",
    "    (\"Minatures\",\"Category\",\"Products that are classified under the Minatures category\",37),\n",
    "    (\"Party Games\", \"Category\", \"Products that are classified under the Party Games category\",47),\n",
    "]\n",
    "promo_groups = pl.DataFrame(promo_groups,schema=promo_groups_schema,orient=\"row\").with_row_index(\"promotion_group_id\",1).with_columns(\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ")\n",
    "promo_groups.write_parquet(Path.cwd() / \"mock_data\" / \"promotion_groups.parquet\")\n",
    "\n",
    "promotion_groups_products = {\n",
    "    \"promotion_group_id\" : [1]*10,\n",
    "    \"product_id\" : list(range(1,11)),\n",
    "    \"date_added\" : [set_date]*10,\n",
    "    \"date_removed\" : [None]*10,\n",
    "    \"date_modified\" : [set_date]*10,\n",
    "    \"date_deleted\" : [None]*10,\n",
    "}\n",
    "\n",
    "pl.from_dict(promotion_groups_products).write_parquet(Path.cwd() / \"mock_data\" / \"promotion_groups_products.parquet\")\n",
    "\n",
    "channel_schema = [\"channel_uuid\",\"channel_name\",\"channel_type\",\"description\"]\n",
    "channels = [\n",
    "    [\"Main Website\", \"Online\", \"The primary e-commerce website for all global sales and customer engagement.\"],\n",
    "    [\"Flagship Store - Downtown\", \"In-Store\", \"The main physical retail brick-and-mortar location in the city center.\"],\n",
    "    [\"Official Mobile App\",\"Mobile App\",\"The official shopping application for both iOS and Android platforms.\"],\n",
    "    [\"Amazon Marketplace US\",\"Third-Party\",\"Products sold through the Amazon US third-party marketplace platform\"]\n",
    "]\n",
    "for i, e in enumerate(channels):\n",
    "    e.insert(0,str(uuid.uuid5(NAMESPACE_CHANNELS, str(i))))\n",
    "\n",
    "channels_df = pl.DataFrame(channels, schema=channel_schema, orient=\"row\").with_row_index(\"channel_id\",1).with_columns(\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ").write_parquet(Path.cwd() / \"mock_data\"/ \"channels.parquet\")\n",
    "\n",
    "payment_types_schema = [\"payment_type_uuid\",\"type_name\",\"description\"]\n",
    "payment_types = [\n",
    "    [\"Credit Card\", \"Payment made with a major credit card network (e.g., Visa, Mastercard, American Express).\"],\n",
    "    [\"Debit Card\", \"Payment made with a bank-issued debit card, drawing funds directly from a checking account.\"],\n",
    "    [\"Gift Card\", \"Payment made using a prepaid gift card issued by the retailer. The transaction deducts from the card's balance.\"],\n",
    "    [\"PayPal\", \"Payment processed through the PayPal digital wallet service.\"],\n",
    "    [\"Digital Wallet\", \"Contactless payment made using a digital wallet service (Apple Pay, Google Pay, Samsung Wallet, etc.).\"],\n",
    "    [\"Cash\", \"Payment made with physical currency (banknotes and coins), typically used for in-store transactions.\"],\n",
    "    [\"Klarna\", \"A 'Buy Now, Pay Later' service that allows customers to pay for their purchase in installments.\"],\n",
    "    [\"Store Credit\", \"Credit applied to a customer's account, typically from a product return, used as full or partial payment.\"],\n",
    "]\n",
    "for i, e in enumerate(payment_types):\n",
    "    e.insert(0,str(uuid.uuid5(NAMESPACE_PAYMENT_TYPES, str(i))))\n",
    "\n",
    "payment_types_df = pl.DataFrame(payment_types,schema=payment_types_schema, orient=\"row\").with_row_index(\"payment_type_id\",1).with_columns(\n",
    "    pl.lit(set_date).alias(\"date_added\"),\n",
    "    pl.lit(None).alias(\"date_removed\"),\n",
    "    pl.lit(set_date).alias(\"date_modified\"),\n",
    "    pl.lit(None).alias(\"date_deleted\"),\n",
    ").write_parquet(Path.cwd() / \"mock_data\" / \"payment_types.parquet\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
